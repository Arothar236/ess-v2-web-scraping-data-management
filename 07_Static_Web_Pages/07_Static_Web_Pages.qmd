---
title: "Session 7: Scraping Static Web Pages"
subtitle: "Introduction to Web Scraping and Data Management for Social Scientists"
author: "Johannes B. Gruber"
date: 2024-07-30
from: markdown+emoji
format:
  revealjs:
    smaller: true
    width: 1600
    height: 900
    scrollable: true
    code-line-numbers: true
    slide-number: c/t
    logo: ../ess_logo.png
    embed-resources: true
bibliography: ../references.bib
execute:
  cache: false
  echo: true
engine: knitr
highlight-style: nord
---

# Introduction
## This Course

<center>
```{r setup}
#| echo: false
#| message: false
library(tinytable)
library(tidyverse)
tibble::tribble(
  ~Day, ~Session,
  1,  "Introduction",
  2,  "Data Structures and Wrangling",
  3,  "Working with Files",
  4,  "Linking and joining data & SQL",
  5,  "Scaling, Reporting and Database Software",
  6,  "Introduction to the Web",
  7,  "Static Web Pages",
  8,  "Application Programming Interface (APIs) ",
  9,  "Interactive Web Pages",
  10, "Building a Reproducible Research Project",
) |> 
  tt() |> 
  style_tt() |> 
  style_tt(i = 7, background = "#FDE000")
```
</center>

## The Plan for Today

:::: {.columns}

::: {.column width="60%"}
In this session, we trap some **docile data** that wants to be found.
We will:

- Go over some parsing examples:
  - Wikipedia: World World Happiness Report
- Discuss some examples of good approaches to data wrangling
- Go into a bit more detail on requesting raw data
  
![Original Image Source: prowebscraper.com](media/web_scraping_steps.png)
:::

::: {.column width="40%" }
![](https://images.unsplash.com/photo-1534361960057-19889db9621e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1740&q=80)
[Joe Caione](https://unsplash.com/@joeyc) via unsplash.com
:::

::::


```{css}
#| echo: false

.datatables {
  font-size: smaller;
}

```

# Example: World Happiness Report
## Use your Browser to Scout

:::: {.columns}

::: {.column width="45%"}
[
  ![](media/en.wikipedia.org_wiki_World_Happiness_Report.png)
](https://en.wikipedia.org/w/index.php?title=World_Happiness_Report&oldid=1165407285)
![](media/en.wikipedia.org_wiki_World_Happiness_Report_table.png)
:::

::: {.column width="50%" }
![](media/en.wikipedia.org_wiki_World_Happiness_Report_code.png)
:::

::::

## Use your Browser's `Inspect` tool

![](media/inspect-view.png)

*Note: Might not be available on all browsers; use Chromium-based or Firefox.*

## Use `rvest` to scrape

:::: {.columns}

::: {.column width="45%"}
```{r}
library(rvest)
library(tidyverse)

# 1. Request & collect raw html
html <- read_html("https://en.wikipedia.org/w/index.php?title=World_Happiness_Report&oldid=1165407285")

# 2. Parse
happy_table <- html |> 
  html_elements(".wikitable") |> # select the right element
  html_table() |>                # special function for tables
  pluck(3)                       # select the third table

# 3. No wrangling necessary
happy_table
```
:::

::: {.column width="50%" }
```{r}
## Plot relationship wealth and life expectancy
ggplot(happy_table, aes(x = `GDP per capita`, y = `Healthy life expectancy`)) + 
  geom_point() + 
  geom_smooth(method = 'lm')
```

:::
::::

## Exercises 1

1. Get the table with 2023 opinion polling for the next United Kingdom general election from <https://en.wikipedia.org/wiki/Opinion_polling_for_the_2024_United_Kingdom_general_election>
2. Wrangle and plot the data opinion polls

# Example: UK prime ministers on Wikipedia
## Use your Browser to Scout

[
  ![](media/list-pms.png)
](https://en.wikipedia.org/w/index.php?title=List_of_prime_ministers_of_the_United_Kingdom&oldid=1166167337)

## Use `rvest` to scrape

:::: {.columns}

::: {.column width="45%"}
```{r}
# 1. Request & collect raw html
html <- read_html("https://en.wikipedia.org/w/index.php?title=List_of_prime_ministers_of_the_United_Kingdom&oldid=1166167337") # I'm using an older version of the site since some just changed it

# 2. Parse
pm_table <- html |> 
  html_element(".wikitable:contains('List of prime ministers')") |>
  html_table() |> 
  as_tibble(.name_repair = "unique") |> 
  filter(!duplicated(`Prime ministerOffice(Lifespan)`))

# 3. No wrangling necessary
pm_table
```
:::

::: {.column width="50%" }

```
<td rowspan="4">
  <span class="anchor" id="18th_century"></span>
   <b>
     <a href="/wiki/Robert_Walpole" title="Robert Walpole">Robert Walpole</a>
   </b>
   <sup id="cite_ref-FOOTNOTEEccleshallWalker20021,_5EnglefieldSeatonWhite19951–5PrydeGreenwayPorterRoy199645–46_28-0" class="reference">
     <a href="#cite_note-FOOTNOTEEccleshallWalker20021,_5EnglefieldSeatonWhite19951–5PrydeGreenwayPorterRoy199645–46-28">[27]</a>
   </sup>
   <br>
   <span style="font-size:85%;">MP for <a href="/wiki/King%27s_Lynn_(UK_Parliament_constituency)" title="King's Lynn (UK Parliament constituency)">King's Lynn</a>
   <br>(1676–1745)
  </span>
</td>
```

```{r}
links <- html |> 
  html_elements(".wikitable:contains('List of prime ministers') b a") |>
  html_attr("href")
title <- html |> 
  html_elements(".wikitable:contains('List of prime ministers') b a") |>
  html_text()
tibble(name = title, link = links)
```

Note: these are relative links that need to be combined with *https://en.wikipedia.org/* to work
:::
::::


## Exercises 2

1. For extracting text, `rvest` has two functions: `html_text` and `html_text2`. Explain the difference. You can test your explanation with the example html below.

```{r}
html <- "<p>This is some text
         some more text</p><p>A new paragraph!</p>
         <p>Quick Question, is web scraping:

         a) fun
         b) tedious
         c) I'm not sure yet!</p>" |> 
  read_html()
```

2. How could you convert the `links` objects so that it contains actual URLs?
3. How could you add the links we extracted above to the `pm_table` to keep everything together?

# Example: Getting content from embedded json


```{r}
html <- read_html("https://news.sky.com/story/crowdstrike-company-that-caused-global-techno-meltdown-offers-partners-10-vouchers-to-say-sorry-and-they-dont-work-13184488")

data <- html %>%
  rvest::html_element("[type=\"application/ld+json\"]") %>%
  rvest::html_text() %>%
  jsonlite::fromJSON()

datetime <- data$datePublished %>%
  lubridate::as_datetime()

# headline
headline <- data$headline

# author
author <- data$author$name

text <- html %>%
  rvest::html_elements(".sdc-article-body p") %>%
  rvest::html_text2() %>%
  paste(collapse = "\n")
```


## Exercises 3

1. Get the author, publication datetime, headline and text from this site: <https://www.cnet.com/tech/services-and-software/facebook-hopes-to-normalize-idea-of-data-scraping-leaks-says-leaked-internal-memo/> (hint: it works in a very similar way, but you have to apply one extra data wrangling step)

# Example: zeit.de
## Special Requests: Behind Paywall

Let's get this [cool data journalism article](https://www.zeit.de/mobilitaet/2024-04/deutschlandticket-klimaschutz-oeffentliche-verkehrsmittel-autos-verkehrswende).

```{r}
html <- read_html("https://www.zeit.de/mobilitaet/2024-04/deutschlandticket-klimaschutz-oeffentliche-verkehrsmittel-autos-verkehrswende")
html |> 
  html_elements(".article-body p") |> 
  html_text2()
```

:::{.fragment}
:thinking: Wait, that's only the first two paragraphs!
:::

:::{.fragment}
:bulb: Websites use cookies to remember users (including logged in ones)
:::

## What are browser cookies

- Small pieces of data stored on the user's device by the web browser while browsing websites
- **Purpose**:
  - **Session Management**: Maintain user sessions by storing login information and keeping users logged in as they navigate a website.
  - **Personalization**: Save user preferences, such as language settings or theme choices, to enhance user experience.
  - **Tracking and Analytics**: Track user behavior across websites for analytics and targeted advertising.
- We can use them in scraping:
  - to get content from websites that require consent before giving access
  - to authenticate as a user with content access privileges
  - to access personalized content
  - to simulate real user behavior, reducing the chances of getting blocked by websites with anti-scraping measures
- You can use browser extensions like [“Get
cookies.txt”](https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc) for Chromium-based browsers or
[“cookies.txt”](https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/) for Firefox to save your cookies to a file
- Implications:
  - You need to keep cookies secure as they can authenticate others as you!

## Special Requests: ~~Behind Paywall~~ Cookies!

```{r}
#| eval: false
library(cookiemonster)
add_cookies("cookies.txt")
```

```{r}
#| eval: false
html <- request("https://www.zeit.de/mobilitaet/2024-04/deutschlandticket-klimaschutz-oeffentliche-verkehrsmittel-autos-verkehrswende") |> # start a request
  req_options(cookie = get_cookies("zeit.de", as = "string")) |> # add cookies to be sent with it
  req_perform() |> 
  resp_body_html() # extract html from response

html |> 
  html_elements(".article-body p") |> 
  html_text2()
```


# Example: South African Parliament (a special case)

```{r}
library(httr2)
html <- request("https://web.archive.org/web/20240519142346/https://www.parliament.gov.za/register-members-Interests") |> 
  req_timeout(100) |> 
  req_perform() |> 
  resp_body_html()

links <- html |> 
  html_elements(".parly-h2+ul a") |> 
  html_attr("href")

years <- html |> 
  html_elements(".parly-h2+ul a") |> 
  html_text()

dir.create("data/za", showWarnings = FALSE)
interest_pdfs <- tibble(
  link = links, year = years
) |> 
  mutate(file_name = paste0("data/za/", year, ".pdf"))

if (!file.exists("data/za/2018.pdf")) {
  curl::multi_download(
    urls = interest_pdfs$link, 
    destfiles = interest_pdfs$file_name
  )
}

```



## What do we want

- REGISTER OF MEMBERS' INTERESTS

## Scraping data from PDFs?

:::: {.columns}

::: {.column width="45%"}
- Data inside a PDF is actually not such an uncommon case
- Many institutions share PDFs with tables, images and lists of data
- We can use some of our new pattern finding skills to scrape data from these PDFs as well though
  - Session names seem to be in a larger font and bold
  - Paper titles are in italics
  - Authors are either bold or plain font
:::

::: {.column width="50%"}
![](media/pdf.png)
:::

::::

## Let's investigate the PDF a little

```{r}
library(pdftools)
comptext <- pdf_data("data/za/2018.pdf", font_info = TRUE)
comptext[[2]]
```

We see here that:

- each page is an element in a list
- each word is in one row of the table
- it contains the font_size and font_name
- the position of each word on tha page is given with x and y coordinates


:::{.fragment}
Let's investigate a few words we saw above:

```{r}
# a politician name
comptext[[2]] |> 
  filter(str_detect(text, "Abrahams,"))

# an item header
comptext[[2]] |> 
  filter(str_detect(text, "1"))

# a disclose
comptext[[2]] |> 
  filter(str_detect(text, "disclose"))

# a word inside table
comptext[[2]] |> 
  filter(str_detect(text, "Pringle"))

# a table header
comptext[[2]] |> 
  filter(str_detect(text, "Description"))
```

- It looks like we can say relatively easily where a new politician entry starts based on the font
- The item header has the same font name, but a different size
- We can tell quite easily on which items there is nothing to disclose
- The table colnames are similar to item headers, but start at a different x location
:::


```{r}
#| eval: false
p1 <- comptext[[2]]
p1 |> 
  filter(font_name == "Arial-BoldMT", round(font_size, 1) == 7.5,
         x < 56) |> View()
```


## Exercises 4

1. In the folder /data (relative to this document) there is a PDF with some text. Read it into R
2. The PDF has two columns, bring the text in the right order as a human would read it
3. Let's assume you wanted to have this text in a table with one column indicating the section and one having the text of the section
4. Now let's assume you wanted to parse this on the paragraph level instead



# Optional Homework

You have seen some tools and tricks to scrape websites now.
But your best ally in web scraping is **experience**!
Until tomorrow noon, your task is to find a page on Wikipedia you find interesting and scrape content from there.
Even if you don't fully succeed, document the steps you take and note down where the information can be found.
If you want to try to get some data you actuall need from a different website, your're also welcome.
But note that if you collect raw html in R and the data is not where it should be (e.g., the html elements containing panel names do not exist), you might have discovered a more advanced site, which we will cover later. 
Note that down and try another conference.

Deadline: Friday before class


# Wrap Up

Save some information about the session for reproducibility.

```{r}
#| code-fold: true
#| code-summary: "Show Session Info"
sessionInfo()
```
